{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnYGd4lAWMDYDTX2FozAo4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"acfS-cldphvN"},"outputs":[],"source":["!pip install -qqq datasets transformers trl peft accelerate bitsandbytes wandb --progress-bar off\n","# STEP 0: Confirm GPU is available\n","# !nvidia-smi\n","\n","# STEP 1: Install dependencies\n","#!pip install -U ninja packaging\n","#!pip install triton --quiet\n","\n","#!pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.5.6"]},{"cell_type":"code","source":["import torch\n","from datasets import load_dataset\n","from peft import LoraConfig, get_peft_model\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from trl import GRPOConfig, GRPOTrainer\n","\n","import re\n","import io\n","import contextlib\n","\n","import wandb\n","\n","wandb.login()\n"],"metadata":{"id":"ywcIq-SaqbUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset\n","\n","# Define the formatted prompt\n","prompt = (\n","    \"Write a Python program that is difficult for another model trained on Qwen/Qwen2.5-Coder-0.5B data to predict. \"\n","    \"The program should return a singular integer value. \"\n","    \"Then, show only the exact output of running your program.\\n\\n\"\n","\n","    \"Format your response exactly like these examples:\\n\\n\"\n","\n","    \"```python\\n\"\n","    \"def tricky():\\n\"\n","    \"    return int('0b1011', 2)\\n\"\n","    \"print(tricky())\\n\"\n","    \"```\\n\"\n","    \"```output\\n\"\n","    \"11\\n\"\n","    \"```\\n\\n\"\n","\n","    \"```python\\n\"\n","    \"def f():\\n\"\n","    \"    return sum([i % 3 for i in range(10)])\\n\"\n","    \"print(f())\\n\"\n","    \"```\\n\"\n","    \"```output\\n\"\n","    \"10\\n\"\n","    \"```\\n\\n\"\n","\n","    \"Now you try:\\n\"\n",")\n","\n","# Create a dataset of 1000 identical prompts\n","dataset = Dataset.from_dict({\n","    \"prompt\": [prompt] * 1000\n","})\n","\n","print(dataset)\n"],"metadata":{"id":"mxBiTpywqdgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_id = \"Qwen/Qwen2.5-Coder-0.5B\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=\"auto\",\n","    device_map=\"auto\",\n","    # attn_implementation=\"flash_attention_2\", # Removed this line\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","'''\n","model_id = \"Qwen/Qwen2.5-Coder-0.5B\"\n","model2 = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=\"auto\",\n","    device_map=\"auto\",\n","    # attn_implementation=\"flash_attention_2\", # Removed this line\n",")\n","tokenizer2 = AutoTokenizer.from_pretrained(model_id)\n","'''\n","\n","# Load LoRA\n","lora_config = LoraConfig(\n","    task_type=\"CAUSAL_LM\",\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=\"all-linear\",\n",")\n","model = get_peft_model(model, lora_config)\n","#model2 = get_peft_model(model2, lora_config)\n","print(model.print_trainable_parameters())\n","#print(model2.print_trainable_parameters())\n"],"metadata":{"id":"G7vPphl4qmvf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reward_fxn(completions, **kwargs):\n","\n","  rewards = []\n","\n","  for comp in completions:\n","    if not isinstance(comp, str):\n","      rewards.append(-1)  # invalid completion\n","      continue\n","\n","    #Extract Code according to schema\n","    code = re.search(r\"```python\\s*\\n(.*?)```\", comp, re.DOTALL)\n","    if code:\n","      code = code.group(1).strip()\n","    else:\n","      code = \"\"\n","\n","    expected_output = re.search(r\"```output\\s*(.*?)```\", comp, re.DOTALL)\n","    if expected_output:\n","      expected_output = expected_output.group(1).strip()\n","    else:\n","      expected_output = \"\"\n","\n","    prompt2 = (\n","        \"Examine this code and predict the integer output.  \\n\"\n","        f\"{code}\\n\\n\"\n","        \"Do not include any text, markdown, or explanation, just the number.\"\n","    )\n","    model_pred = generator2(prompt2, max_new_tokens=200, do_sample=True, temperature=0.7)[0]['generated_text'] #should be propmted to just return a integer value\n","    model_pred = re.search(r\"\\b(-?\\d+)\\b\", model_pred) # search for first number that appears\n","    model_pred = model_pred.group(1) if model_pred else \"\"\n","    try:\n","      model_pred = int(model_pred)\n","    except:\n","      model_pred = \"ERROR: Conversion to integer failed\"\n","\n","    true = run_and_capture(code)\n","\n","\n","    print(\"---------------------------------------\")\n","    print(f'model_pred: {model_pred}')\n","    print(f'code: {code}')\n","    print(f'expected output: {expected_output}')\n","    print(f'true output: {true}')\n","    print(f'original_completion: {comp}')\n","    print(\"---------------------------------------\")\n","\n","    reward = 1 if model_pred == true else -1\n","    rewards.append(reward)\n","\n","  return rewards\n","\n"],"metadata":{"id":"chKpA4E7qqlj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training arguments\n","training_args = GRPOConfig(\n","    output_dir=\"GRPO\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=2,\n","    max_prompt_length=512,\n","    max_completion_length=96,\n","    num_generations=8,\n","    optim=\"adamw_8bit\",\n","    num_train_epochs=1,\n","    bf16=False, # Changed to False to disable BF16\n","    report_to=[\"wandb\"],\n","    remove_unused_columns=False,\n","    logging_steps=1,\n",")\n","\n","# Trainer\n","trainer = GRPOTrainer(\n","    model=model1,\n","    reward_funcs=[reward_fxn],\n","    args=training_args,\n","    train_dataset=dataset,\n",")\n","\n","# Train model\n","wandb.init(project=\"GRPO\")\n","trainer.train()"],"metadata":{"id":"bmrRXNtMqvZn"},"execution_count":null,"outputs":[]}]}